{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import emmental\n",
    "from emmental import Meta\n",
    "from emmental.learner import EmmentalLearner\n",
    "from emmental.model import EmmentalModel\n",
    "from emmental.scorer import Scorer\n",
    "from emmental.task import EmmentalTask\n",
    "from modules.bert_module import BertModule\n",
    "from task_config import SuperGLUE_LABEL_MAPPING, SuperGLUE_TASK_METRIC_MAPPING\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize Emmental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-05 15:01:41,593][INFO] emmental.meta:95 - Setting logging directory to: logs/2019_06_05/15_01_41\n",
      "[2019-06-05 15:01:41,604][INFO] emmental.meta:56 - Loading Emmental default config from /dfs/scratch0/bradenjh/emmental/src/emmental/emmental-default-config.yaml.\n",
      "[2019-06-05 15:01:41,604][INFO] emmental.meta:143 - Updating Emmental config from user provided config.\n"
     ]
    }
   ],
   "source": [
    "emmental.init(\n",
    "    \"logs\",\n",
    "    config={\n",
    "        \"model_config\": {\"device\": 0, \"dataparallel\": False},\n",
    "        \"learner_config\": {\n",
    "            \"n_epochs\": 4,\n",
    "            \"valid_split\": \"val\",\n",
    "            \"optimizer_config\": {\"optimizer\": \"adam\", \"lr\": 1e-5},\n",
    "            \"min_lr\": 0,\n",
    "            \"lr_scheduler_config\": {\n",
    "                \"warmup_percentage\": 0.1,\n",
    "                \"lr_scheduler\": None,\n",
    "            },\n",
    "        },\n",
    "        \"logging_config\": {\n",
    "            \"counter_unit\": \"batch\",\n",
    "            \"evaluation_freq\": 100,\n",
    "            \"checkpointing\": True,\n",
    "            \"checkpointer_config\": {\n",
    "                \"checkpoint_metric\": {\"WiC/SuperGLUE/val/accuracy\":\"max\"},\n",
    "                \"checkpoint_freq\": 1,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta_config': {'seed': 0, 'verbose': True, 'log_path': None},\n",
       " 'model_config': {'model_path': None, 'device': 0, 'dataparallel': False},\n",
       " 'learner_config': {'fp16': False,\n",
       "  'n_epochs': 4,\n",
       "  'train_split': 'train',\n",
       "  'valid_split': 'val',\n",
       "  'test_split': 'test',\n",
       "  'ignore_index': 0,\n",
       "  'optimizer_config': {'optimizer': 'adam',\n",
       "   'lr': 1e-05,\n",
       "   'l2': 0.0,\n",
       "   'grad_clip': 1.0,\n",
       "   'sgd_config': {'momentum': 0.9},\n",
       "   'adam_config': {'betas': (0.9, 0.999)}},\n",
       "  'lr_scheduler_config': {'lr_scheduler': None,\n",
       "   'warmup_steps': None,\n",
       "   'warmup_unit': 'batch',\n",
       "   'warmup_percentage': 0.1,\n",
       "   'min_lr': 0.0,\n",
       "   'linear_config': {'min_lr': 0.0},\n",
       "   'exponential_config': {'gamma': 0.9},\n",
       "   'plateau_config': {'factor': 0.5, 'patience': 10, 'threshold': 0.0001}},\n",
       "  'task_scheduler': 'round_robin',\n",
       "  'global_evaluation_metric_dict': None,\n",
       "  'min_lr': 0},\n",
       " 'logging_config': {'counter_unit': 'batch',\n",
       "  'evaluation_freq': 100,\n",
       "  'writer_config': {'writer': 'tensorboard', 'verbose': True},\n",
       "  'checkpointing': True,\n",
       "  'checkpointer_config': {'checkpoint_path': None,\n",
       "   'checkpoint_freq': 1,\n",
       "   'checkpoint_metric': {'WiC/SuperGLUE/val/accuracy': 'max'},\n",
       "   'checkpoint_task_metrics': None,\n",
       "   'checkpoint_runway': 0,\n",
       "   'checkpoint_clear': True}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TASK_NAME = \"WiC\"\n",
    "DATA_DIR = \"/dfs/scratch0/bradenjh/superglue\" #os.environ[\"SUPERGLUEDATA\"]\n",
    "BERT_MODEL_NAME = \"bert-large-cased\"\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract train/dev dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-05 15:01:41,796][INFO] tokenizer:9 - Loading Tokenizer bert-large-cased\n",
      "[2019-06-05 15:01:42,062][INFO] pytorch_pretrained_bert.tokenization:190 - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /afs/cs.stanford.edu/u/bradenjh/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "[2019-06-05 15:01:42,094][INFO] parsers.wic:20 - Loading data from /dfs/scratch0/bradenjh/superglue/WiC/train.jsonl.\n",
      "[2019-06-05 15:01:42,126][INFO] parsers.wic:23 - Sample 0: {'label': False, 'word': 'carry', 'pos': 'V', 'sentence1': 'You must carry your camping gear .', 'sentence2': 'Sound carries well over water .', 'sentence1_idx': '2', 'sentence2_idx': '1', 'idx': 0}\n",
      "[2019-06-05 15:01:42,127][INFO] parsers.wic:23 - Sample 1: {'label': False, 'word': 'go', 'pos': 'V', 'sentence1': 'Messages must go through diplomatic channels .', 'sentence2': 'Do you think the sofa will go through the door ?', 'sentence1_idx': '2', 'sentence2_idx': '6', 'idx': 1}\n",
      "[2019-06-05 15:01:43,973][INFO] parsers.wic:136 - Max token len 68\n",
      "[2019-06-05 15:01:43,976][INFO] dataloaders:46 - Loaded train for WiC with 5428 samples.\n",
      "[2019-06-05 15:01:43,977][INFO] parsers.wic:20 - Loading data from /dfs/scratch0/bradenjh/superglue/WiC/val.jsonl.\n",
      "[2019-06-05 15:01:43,982][INFO] parsers.wic:23 - Sample 0: {'label': False, 'word': 'board', 'pos': 'N', 'sentence1': 'Room and board .', 'sentence2': 'He nailed boards across the windows .', 'sentence1_idx': '2', 'sentence2_idx': '2', 'idx': 0}\n",
      "[2019-06-05 15:01:43,982][INFO] parsers.wic:23 - Sample 1: {'label': False, 'word': 'circulate', 'pos': 'V', 'sentence1': 'Circulate a rumor .', 'sentence2': 'This letter is being circulated among the faculty .', 'sentence1_idx': '0', 'sentence2_idx': '4', 'idx': 1}\n",
      "[2019-06-05 15:01:44,211][INFO] parsers.wic:136 - Max token len 57\n",
      "[2019-06-05 15:01:44,212][INFO] dataloaders:46 - Loaded val for WiC with 638 samples.\n",
      "[2019-06-05 15:01:44,212][INFO] parsers.wic:20 - Loading data from /dfs/scratch0/bradenjh/superglue/WiC/test.jsonl.\n",
      "[2019-06-05 15:01:44,221][INFO] parsers.wic:23 - Sample 0: {'word': 'defeat', 'pos': 'N', 'sentence1': 'It was a narrow defeat .', 'sentence2': \"The army 's only defeat .\", 'sentence1_idx': '4', 'sentence2_idx': '4', 'idx': 0}\n",
      "[2019-06-05 15:01:44,221][INFO] parsers.wic:23 - Sample 1: {'word': 'groom', 'pos': 'V', 'sentence1': 'Groom the dogs .', 'sentence2': 'Sheila groomed the horse .', 'sentence1_idx': '0', 'sentence2_idx': '1', 'idx': 1}\n",
      "[2019-06-05 15:01:44,721][INFO] parsers.wic:136 - Max token len 60\n",
      "[2019-06-05 15:01:44,723][INFO] dataloaders:46 - Loaded test for WiC with 1400 samples.\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import get_dataloaders\n",
    "\n",
    "dataloaders = get_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    task_name=TASK_NAME,\n",
    "    splits=[\"train\", \"val\", \"test\"],\n",
    "    max_sequence_length=256,\n",
    "    max_data_samples=None,\n",
    "    tokenizer_name=BERT_MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-05 15:02:41,675][INFO] slicing.slicing_function:38 - Total 569 / 1400 examples are in slice slice_verb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([False,  True, False,  ..., False,  True,  True], dtype=torch.bool),\n",
       " tensor([0, 1, 0,  ..., 0, 1, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from slicing.WiC_slices import slice_func_dict\n",
    "\n",
    "f = slice_func_dict[\"slice_verb\"]\n",
    "f(dataloaders[2].dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Emmental task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(task_name, immediate_ouput_dict, Y, active):\n",
    "    module_name = f\"{task_name}_pred_head\"\n",
    "    return F.cross_entropy(\n",
    "        immediate_ouput_dict[module_name][0][active], (Y.view(-1) - 1)[active]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(task_name, immediate_ouput_dict):\n",
    "    module_name = f\"{task_name}_pred_head\"\n",
    "    return F.softmax(immediate_ouput_dict[module_name][0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(golds, probs, preds):\n",
    "    return {\"macro_f1\": f1_score(golds, preds, average=\"macro\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModule(nn.Module):\n",
    "    def __init__(self, feature_dim, class_cardinality):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(feature_dim, class_cardinality)\n",
    "\n",
    "    def forward(self, feature, idx1, idx2):\n",
    "        last_layer = feature[-1]\n",
    "        emb = last_layer[:,0,:]\n",
    "        idx1 = idx1.unsqueeze(-1).unsqueeze(-1).expand([-1, -1, last_layer.size(-1)])\n",
    "        idx2 = idx2.unsqueeze(-1).unsqueeze(-1).expand([-1, -1, last_layer.size(-1)])\n",
    "        word1_emb = last_layer.gather(dim=1, index=idx1).squeeze(dim=1)\n",
    "        word2_emb = last_layer.gather(dim=1, index=idx2).squeeze(dim=1)\n",
    "        input = torch.cat([emb, word1_emb, word2_emb], dim=-1)\n",
    "        return self.linear.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_OUTPUT_DIM = 768 if \"base\" in BERT_MODEL_NAME else 1024\n",
    "TASK_CARDINALITY = (\n",
    "    len(SuperGLUE_LABEL_MAPPING[TASK_NAME].keys())\n",
    "    if SuperGLUE_LABEL_MAPPING[TASK_NAME] is not None\n",
    "    else 1\n",
    ")\n",
    "\n",
    "emmental_task = EmmentalTask(\n",
    "    name=TASK_NAME,\n",
    "    module_pool=nn.ModuleDict(\n",
    "        {\n",
    "            \"bert_module\": BertModule(BERT_MODEL_NAME),\n",
    "            f\"{TASK_NAME}_pred_head\": LinearModule(3 * BERT_OUTPUT_DIM, TASK_CARDINALITY),\n",
    "        }\n",
    "    ),\n",
    "    task_flow=[\n",
    "        {\n",
    "            \"name\": \"input\",\n",
    "            \"module\": \"bert_module\",\n",
    "            \"inputs\": [(\"_input_\", \"token_ids\"), (\"_input_\", \"token_segments\")],\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"{TASK_NAME}_pred_head\",\n",
    "            \"module\": f\"{TASK_NAME}_pred_head\",\n",
    "            \"inputs\": [(\"input\", 0), (\"_input_\", \"sent1_idxs\"), (\"_input_\", \"sent2_idxs\")],\n",
    "        },\n",
    "    ],\n",
    "    loss_func=partial(ce_loss, TASK_NAME),\n",
    "    output_func=partial(output, TASK_NAME),\n",
    "    scorer=Scorer(\n",
    "        metrics=SuperGLUE_TASK_METRIC_MAPPING[TASK_NAME]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_model = EmmentalModel(name=\"SuperGLUE_single_task\", tasks=[emmental_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmental_learner = EmmentalLearner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# emmental_learner.learn(mtl_model, dataloaders.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_model.score(dataloaders[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl_model.score(dataloaders[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PKL_PATH = \"/dfs/scratch0/bradenjh/emmental-tutorials/superglue/models/WiC_verb_trigram_v2.pth\"\n",
    "# PKL_PATH = \"/dfs/scratch0/bradenjh/emmental-tutorials/superglue/logs/2019_05_29/13_59_55/best_model_WiC_SuperGLUE_val_accuracy.pth\"\n",
    "PKL_PATH = \"/dfs/scratch0/bradenjh/emmental-tutorials/superglue/logs/2019_06_04/11_01_21/best_model_WiC_SuperGLUE_val_accuracy.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl_model.save(PKL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = EmmentalModel(name=\"SuperGLUE_single_task\", tasks=[emmental_task])\n",
    "new_model.load(PKL_PATH)\n",
    "new_model.score(dataloaders[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from task_config import (\n",
    "    SuperGLUE_LABEL_MAPPING, \n",
    "    SuperGLUE_TASK_METRIC_MAPPING, \n",
    "    SuperGLUE_TASK_SPLIT_MAPPING\n",
    ")\n",
    "\n",
    "SPLIT = \"val\"\n",
    "\n",
    "def make_analysis_df(model):\n",
    "    # Get predictions\n",
    "    gold_dict, prob_dict, pred_dict = model.predict(dataloaders[SPLIT], return_preds=True)\n",
    "    probs = prob_dict[\"WiC\"][:,0]\n",
    "    preds = pred_dict[\"WiC\"]\n",
    "\n",
    "    # Load raw data\n",
    "    jsonl_path = os.path.join(\n",
    "        DATA_DIR, TASK_NAME, SuperGLUE_TASK_SPLIT_MAPPING[TASK_NAME][SPLIT]\n",
    "    )\n",
    "\n",
    "    # Add new columns\n",
    "    rows = [json.loads(row) for row in open(jsonl_path, encoding=\"utf-8\")]\n",
    "    for i, row in enumerate(rows):\n",
    "        row[\"prob\"] = probs[i]\n",
    "        row[\"pred\"] = True if preds[i] == 1 else False\n",
    "        row[\"correct\"] = \"Y\" if row[\"pred\"] == row[\"label\"] else \"N\"\n",
    "\n",
    "    # Make tsv\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df[['idx', 'label', 'pred', 'prob', 'correct', 'word', 'pos', \n",
    "             'sentence1_idx', 'sentence2_idx',\n",
    "             'sentence1', 'sentence2']]\n",
    "    return df\n",
    "\n",
    "df = make_analysis_df(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUTORIALS_ROOT = \"/dfs/scratch0/bradenjh/emmental-tutorials/\"\n",
    "\n",
    "# out_path = os.path.join(TUTORIALS_ROOT, \"superglue\", \"analysis\", f\"WiC_{SPLIT}_analysis_v0.csv\")\n",
    "# df.to_csv(out_path)\n",
    "# print(f\"Wrote error analysis to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords as nltk_stopwords\n",
    "# stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "def get_ngrams(tokens, window=1):\n",
    "    num_ngrams = len(tokens) - window + 1\n",
    "    for i in range(num_ngrams):\n",
    "        yield tokens[i:i+window]\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    target = row[\"word\"]\n",
    "    labels.append(1 if row[\"label\"] == True else 2)\n",
    "    sent1_target = row[\"sentence1\"].split()[int(row[\"sentence1_idx\"])]\n",
    "    sent2_target = row[\"sentence2\"].split()[int(row[\"sentence2_idx\"])]\n",
    "    if sent1_target.lower() != sent2_target.lower():\n",
    "        print(index, sent1_target.lower(), sent2_target.lower())\n",
    "#     print(row)\n",
    "#     print(word, )\n",
    "#     for sent in [\"sentence1\", \"sentence2\"]:\n",
    "#         tokens = row[sent].split()\n",
    "#         for i, tok in enumerate(tokens):\n",
    "#             if target in tok:\n",
    "#                 idx = i\n",
    "#                 break\n",
    "#         trigrams.append([' '.join(ngram) \n",
    "#                          for ngram in get_ngrams(tokens[idx-2:idx+2], window=3) \n",
    "#                          if len(ngram) == 3])\n",
    "#     if (set(trigrams[0]).intersection(set(trigrams[1]))):\n",
    "#         preds.append(1)\n",
    "#         print(trigrams)\n",
    "#         print(f\"{target}: {row['sentence1']}.....{row['sentence2']}\")\n",
    "#         print()        \n",
    "#     else:\n",
    "#         preds.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.metrics import *\n",
    "\n",
    "print(accuracy_score(labels, preds, ignore_in_pred=[0]))\n",
    "print(coverage_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
